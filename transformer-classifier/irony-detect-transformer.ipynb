{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from json import JSONDecoder\n",
    "import numpy as np\n",
    "import copy\n",
    "import time\n",
    "import math\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "数据来源：  \n",
    "http://alt.qcri.org/semeval2018/index.php?id=tasks  \n",
    "data文件夹下已处理好word embedding和sequence的部分特征\n",
    "\n",
    "参考了BERT-pytorch其中的encoder部分实现\n",
    "\n",
    "模型结构：\n",
    "sequences ==> Encoder: 2*(self-atten --> add&norm --> feed-forward --> add&norm) ==> fc ==> softmax ==> predicted logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "RANDOM_SEED = 42\n",
    "np.random.seed(RANDOM_SEED)\n",
    "torch.manual_seed(RANDOM_SEED)\n",
    "torch.cuda.manual_seed(RANDOM_SEED)\n",
    "torch.backends.cudnn.deterministic=True "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_folder = \"data\"\n",
    "trainA_file = os.path.join(data_folder, 'trainA.json')\n",
    "trainB_file = os.path.join(data_folder, 'trainB.json')\n",
    "test_file = os.path.join(data_folder, 'test.json')\n",
    "\n",
    "train_feature_f = os.path.join(data_folder, 'task3_train_feature.txt')\n",
    "test_feature_f = os.path.join(data_folder, 'task3_test_feature.txt')\n",
    "\n",
    "word2idx_f = os.path.join(data_folder, 'word2idx.json')\n",
    "pos2idx_f = os.path.join(data_folder, 'pos2idx.json')\n",
    "word_embeds_f = os.path.join(data_folder, 'word_embedding.npy')\n",
    "pos_embeds_f = os.path.join(data_folder, 'pos_embedding.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(filename):\n",
    "    with open(filename, 'r') as file:\n",
    "        data = json.load(file)\n",
    "        dataList=data['data']\n",
    "    return dataList\n",
    "\n",
    "def load_seq_feats(filename):\n",
    "    with open(filename,'r')as f:\n",
    "        test_feature=f.read()\n",
    "    test_feature=JSONDecoder().decode(test_feature)\n",
    "    return test_feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#train: 3450 #valud: 384 #test: 784\n"
     ]
    }
   ],
   "source": [
    "trainA = load_data(trainA_file)[:3450]\n",
    "validA = load_data(trainA_file)[3450:]\n",
    "trainB = load_data(trainB_file)[:3450]\n",
    "validB = load_data(trainB_file)[3450:]\n",
    "test = load_data(test_file)\n",
    "print(\"#train:\", len(trainA),\"#valud:\", len(validA),\"#test:\", len(test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_feature = load_seq_feats(train_feature_f)\n",
    "test_feature = load_seq_feats(test_feature_f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "load word and pos embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dict(fname):\n",
    "    with open(fname, 'r') as file:\n",
    "        dictionary = json.load(file)\n",
    "    return dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab size: 12656\n"
     ]
    }
   ],
   "source": [
    "word2idx = load_dict(word2idx_f)\n",
    "pos2idx = load_dict(pos2idx_f)\n",
    "vocab_size = len(word2idx)\n",
    "print(\"vocab size:\", vocab_size)\n",
    "\n",
    "word_embedding = np.load(word_embeds_f)\n",
    "pos_embedding = np.load(pos_embeds_f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(12656, 700) (28, 28)\n"
     ]
    }
   ],
   "source": [
    "print(word_embedding.shape, pos_embedding.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Positional Encoding  \n",
    "$P E_{2 i}(p)=\\sin \\left(p / 10000^{2 i / d_{model}}\\right)$   \n",
    "$P E_{2 i+1}(p)=\\cos \\left(p / 10000^{2 i / d_{model}}\\right)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 返回的是x+pe\n",
    "def positional_enc(x, dim_model, max_len=5000):\n",
    "    base = 10000\n",
    "    sentence_len = x.size(1)\n",
    "    pe_vec = torch.zeros(max_len, dim_model)\n",
    "    p = torch.arange(0., max_len).unsqueeze(1)\n",
    "    frac = torch.exp(torch.arange(0., dim_model, 2) * -(math.log(10000.0) / dim_model)) \n",
    "    pe_vec[:,0::2] = torch.sin(p)\n",
    "    pe_vec[:,1::2] = torch.cos(p)\n",
    "    pe_vec = pe_vec.unsqueeze(0)\n",
    "    return x.float() + pe_vec[:,:sentence_len]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Attention\n",
    "scaled dot product attention:\n",
    " $\\text { Attention }(Q, K, V)=\\operatorname{softmax}\\left(\\frac{Q K^{T}}{\\sqrt{d_{k}}}\\right) V$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scaled_dot_product_attention(query, key, value, mask=None, dropout=None):\n",
    "    dim_key = query.size(-1)\n",
    "    attn = torch.matmul(query, key.transpose(-2, -1)) / math.sqrt(dim_key)\n",
    "    if mask is not None:\n",
    "        attn = attn.masked_fill(mask == 0, -1e9)\n",
    "    attn_weights = F.softmax(attn, dim = -1)\n",
    "    if dropout is not None:\n",
    "        attn_weights = dropout(attn_weights)\n",
    "    return torch.matmul(attn_weights, value), attn_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Self attention: K=V=Q  \n",
    "each word in the sentence needs to undergo Attention computation, to capture the internal structure of the sentence\n",
    "\n",
    "Multi-head Attention: query, key, and value first go through a linear transformation and then enters into Scaled-Dot Attention. Here, the attention is calculated h times, which allows the model to learn relevant information in different representative child spaces.  \n",
    "When #head=1, it becomes a original self-attention layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadedAttention(nn.Module):\n",
    "    def __init__(self, num_heads, dim_model, dropout=0.1):\n",
    "        super(MultiHeadedAttention, self).__init__()\n",
    "        # make sure input word embedding dimension divides by the number of desired heads\n",
    "        assert dim_model % num_heads == 0\n",
    "        # assume dim of key,query,values are equal\n",
    "        self.dim_qkv = dim_model // num_heads\n",
    "        \n",
    "        self.dim_model = dim_model\n",
    "        self.num_h = num_heads\n",
    "        self.w_q = nn.Linear(dim_model, dim_model) # self.w_qs = nn.Linear(d_model, n_head * d_k) \n",
    "        self.w_k = nn.Linear(dim_model, dim_model) \n",
    "        self.w_v = nn.Linear(dim_model, dim_model)\n",
    "        \n",
    "        self.attn = None\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        self.layer_norm = nn.LayerNorm(dim_model)\n",
    "        \n",
    "    def forward(self, query, key, value, mask=None):\n",
    "        n_batch = query.size(0)\n",
    "        \n",
    "        if mask is not None:\n",
    "#             mask = mask.unsqueeze(1)\n",
    "              mask = mask.view(n_batch,mask.size(1),1,1).expand(n_batch,mask.size(1),self.num_h,self.num_h)\n",
    "        \n",
    "        # linear projections: dim_model => num_h x dim_k \n",
    "        query = self.w_q(query).view(n_batch, -1, self.num_h, self.dim_qkv)\n",
    "        key = self.w_k(key).view(n_batch, -1, self.num_h, self.dim_qkv)\n",
    "        value = self.w_v(value).view(n_batch, -1, self.num_h, self.dim_qkv)\n",
    "        \n",
    "        # Apply attention on all the projected vectors in batch \n",
    "        x, self.attn = scaled_dot_product_attention(query, key, value, mask=mask, \n",
    "                                 dropout=self.dropout)\n",
    "        \n",
    "        # Concat(head1, ..., headh) \n",
    "        x = x.transpose(1, 2).contiguous().view(n_batch, -1, self.num_h * self.dim_qkv)\n",
    "        \n",
    "        x = nn.Linear(dim_model, dim_model, bias=False)(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Position-wise feed forward network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionwiseFeedForward(nn.Module):\n",
    "    def __init__(self, d_model, d_ff, dropout=0.1):\n",
    "        super(PositionwiseFeedForward, self).__init__()\n",
    "        self.w_1 = nn.Linear(d_model, d_ff)\n",
    "        self.w_2 = nn.Linear(d_ff, d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.activation = F.relu # bert uses gelu instead\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.w_2(self.dropout(self.activation(self.w_1(x))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Add & Norm\n",
    "`Residual connection`是对于较为深层的神经网络有比较好的作用，比如网络层很深时，数值的传播随着weight不断的减弱，`Residual connection`是从输入的部分，连到它输出层的部分，把输入的信息原封不动copy到输出的部分，减少信息的损失。\n",
    "`layer-normalization`这种归一化层是为了防止在某些层中由于某些位置过大或者过小导致数值过大或过小，对神经网络梯度回传时有训练的问题，保证训练的稳定性。基本在每个子网络后面都要加上`layer-normalization`、加上`Residual connection`，加上这两个部分能够使深层神经网络训练更加顺利。  \n",
    "(本实验中也许不需要)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AddNorm(nn.Module):\n",
    "    def __init__(self, size, dropout, eps=1e-6):\n",
    "        super(AddNorm, self).__init__()\n",
    "        self.a_2 = nn.Parameter(torch.ones(size))\n",
    "        self.b_2 = nn.Parameter(torch.zeros(size))\n",
    "        self.eps = eps\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    \n",
    "    def forward(self, x, sublayer):\n",
    "        \"Apply residual connection to any sublayer with the same size.\"\n",
    "        x = x.float()\n",
    "        mean = x.mean(-1, keepdim=True)\n",
    "        std = x.std(-1, keepdim=True)\n",
    "        norm = self.a_2 * (x - mean) / (std + self.eps) + self.b_2\n",
    "        return x + self.dropout(sublayer(norm))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Encoder\n",
    "self-attention layers: all of the keys, values and queries come from the previous layer in the encoder. Each position in the encoder can attend to all positions in the previous layer of the encoder.  \n",
    "一层Encoder Layer: self-atten --> add&norm --> feed-forward --> add&norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self, size, attention, feed_forward, dropout=0.1):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "        self.feed_forward = feed_forward\n",
    "        self.self_atten = attention\n",
    "        self.add_norm_1 = AddNorm(size, dropout)\n",
    "        self.add_norm_2 = AddNorm(size, dropout)\n",
    "        self.size = size\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        output = self.add_norm_1(x, lambda x: self.self_atten(x, x, x, mask))\n",
    "        output = self.add_norm_2(output, self.feed_forward)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, layer, N):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.layers = nn.ModuleList([copy.deepcopy(layer) for _ in range(N)]) # clone the layer for N times\n",
    "        self.norm = nn.LayerNorm(layer.size)\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, mask)\n",
    "        return self.norm(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SoftMax(nn.Module):\n",
    "    def __init__(self,n_input,n_out):\n",
    "        super(SoftMax,self).__init__()\n",
    "        self.fc = nn.Linear(n_input,n_out)\n",
    "        self.softmax = nn.LogSoftmax(1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.fc(x)\n",
    "        y = self.softmax(x)\n",
    "#         print(y)\n",
    "        return y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Full Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# single task\n",
    "# embeding --> encoder --> linear --> softmax\n",
    "class SelfAttenClassifier(nn.Module):\n",
    "    def __init__(self, encoder, classifier):\n",
    "        super(SelfAttenClassifier, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.classifier = classifier\n",
    "        \n",
    "    def forward(self, input_embeds, mask, addition_feats=None):\n",
    "        batch_size = input_embeds.size(1)\n",
    "        encoder_out = self.encoder(input_embeds, mask)\n",
    "        feats = encoder_out.sum(dim=1)\n",
    "        \n",
    "        if addition_feats is not None:\n",
    "            feats = torch.cat((feats, addition_feats),dim=1)\n",
    "#         print(encoder_out.size(), feats.size())\n",
    "        outputs = self.classifier(feats)\n",
    "        return outputs,feats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "prepare inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vectorize sequences as inputs \n",
    "def seq_to_tensor(raw_sample,dim_model=728):\n",
    "    seq_embed = torch.tensor([np.concatenate([word_embedding[word2idx[w]],pos_embedding[pos2idx[raw_sample[\"pos\"][i]]]]) \n",
    "                       for i,w in enumerate(raw_sample[\"word\"])])\n",
    "    return seq_embed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class IronyDataset(Dataset):\n",
    "    def __init__(self, raw_data, transform=None, addition_feats=None, with_label=True):\n",
    "        self.data = raw_data\n",
    "        self.transform = transform\n",
    "        self.addition_feats = addition_feats\n",
    "        self.with_label = with_label\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        sample = self.data[index]\n",
    "        label = []\n",
    "        feats = []\n",
    "        if self.with_label:\n",
    "            label = self.data[index][\"label\"]\n",
    "        \n",
    "        if self.transform is not None:\n",
    "            sample = self.transform(sample)\n",
    "            \n",
    "        if self.addition_feats is not None:\n",
    "            feats = self.addition_feats[index]\n",
    "            \n",
    "        return sample, label, feats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "load and pading sequence batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dynamic padding: seqeuences are padded to the maximum length of mini-batch sequences\n",
    "def collate_fn(batch):\n",
    "    sorted_batch = sorted(batch, key=lambda x: x[0].size(0), reverse=True)\n",
    "    sequences = [x[0] for x in sorted_batch]\n",
    "    sequences_padded = torch.nn.utils.rnn.pad_sequence(sequences, batch_first=True)\n",
    "    lengths = torch.LongTensor([len(x) for x in sequences])\n",
    "    labels = torch.LongTensor(list(map(lambda x: x[1], sorted_batch)))\n",
    "    feats = torch.FloatTensor(list(map(lambda x: x[2], sorted_batch)))\n",
    "    return sequences_padded, labels, lengths, feats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def binary_acc(model,loader, device=torch.device('cpu')):\n",
    "    model.eval()\n",
    "    num_corrects = 0\n",
    "    for data in loader:\n",
    "        x, y, lengths, addition_feats = data\n",
    "        x = x.to(device)\n",
    "        y = y.to(device)\n",
    "        lengths = lengths.to(device)\n",
    "        addition_feats = addition_feats.to(device)\n",
    "        \n",
    "#         x = positional_enc(x, dim_model)\n",
    "        maxlen = x.size(1)\n",
    "        mask = torch.arange(maxlen)[None, :].to(device) < lengths[:, None].to(device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            pred,seq_feats = model(x, mask, addition_feats)\n",
    "#         print(torch.max(pred, 1)[1].view(y.size()).data)\n",
    "#         print(y.data)\n",
    "        num_corrects += (torch.max(pred, 1)[1].view(y.size()).data == y.data).sum()\n",
    "    return num_corrects.item() / len(loader.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_loader, loss_func, optimizer, device=torch.device('cpu')):\n",
    "    model.train()\n",
    "    for batch in train_loader:\n",
    "        \n",
    "        x, y, lengths, addition_feats = batch\n",
    "        \n",
    "        x = x.to(device)\n",
    "        y = y.to(device)\n",
    "        lengths = lengths.to(device)\n",
    "        addition_feats = addition_feats.to(device)\n",
    "        \n",
    "#         x = positional_enc(x, dim_model)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        maxlen = x.size(1)\n",
    "        mask = torch.arange(maxlen)[None, :].to(device) < lengths[:, None].to(device)\n",
    "\n",
    "        out, seq_feats = model(x, mask, addition_feats)\n",
    "        \n",
    "        loss = loss_func(out, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TaskA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 16\n",
    "train_data = IronyDataset(trainA, seq_to_tensor, train_feature)\n",
    "train_loader = DataLoader(train_data, batch_size=BATCH_SIZE, shuffle=False, collate_fn=collate_fn)\n",
    "\n",
    "valid_data = IronyDataset(validA, seq_to_tensor, train_feature)\n",
    "valid_loader = DataLoader(valid_data, batch_size=BATCH_SIZE, shuffle=False, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_encoder_layers = 6\n",
    "dim_model=728 # equal to the dim of word embeddings\n",
    "num_heads=8 # dim_model % num_heads == 0\n",
    "d_ff=2912\n",
    "dropout=0.2\n",
    "\n",
    "addition_feats_dim = len(train_feature[0])\n",
    "classfier_input_dim = dim_model+addition_feats_dim\n",
    "\n",
    "# c = copy.deepcopy\n",
    "attn_layer = MultiHeadedAttention(num_heads, dim_model, dropout)\n",
    "ff_layer = PositionwiseFeedForward(dim_model, d_ff, dropout)\n",
    "\n",
    "num_class = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 001, Loss: 2.88112473, Valid Acc: 0.49479167, Duration: 27.12\n",
      "Epoch: 002, Loss: 2.21361995, Valid Acc: 0.50520833, Duration: 27.11\n",
      "Epoch: 003, Loss: 0.64978534, Valid Acc: 0.51822917, Duration: 27.19\n",
      "Epoch: 004, Loss: 0.78552330, Valid Acc: 0.50781250, Duration: 27.20\n",
      "Epoch: 005, Loss: 0.74521846, Valid Acc: 0.55208333, Duration: 27.21\n",
      "Epoch: 006, Loss: 0.69490272, Valid Acc: 0.50520833, Duration: 27.21\n",
      "Epoch: 007, Loss: 0.72938293, Valid Acc: 0.50520833, Duration: 27.37\n",
      "Epoch: 008, Loss: 0.68869430, Valid Acc: 0.52343750, Duration: 27.45\n",
      "Epoch: 009, Loss: 0.68830252, Valid Acc: 0.54166667, Duration: 27.45\n",
      "Epoch: 010, Loss: 0.74929905, Valid Acc: 0.50000000, Duration: 27.45\n"
     ]
    }
   ],
   "source": [
    "model = SelfAttenClassifier(\n",
    "    encoder = Encoder(EncoderLayer(dim_model,attn_layer,ff_layer),num_encoder_layers),\n",
    "    classifier = SoftMax(classfier_input_dim,num_class),\n",
    "    )\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "loss_function = F.nll_loss \n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    torch.set_default_tensor_type(torch.cuda.FloatTensor)\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "if torch.cuda.device_count() > 1:\n",
    "    model = nn.DataParallel(model)\n",
    "model.to(device)\n",
    "\n",
    "time_p, tr_acc_array, ts_acc, loss_p = [], [], [], []\n",
    "epochs = 10\n",
    "# running epoches\n",
    "for epoch in range(1, epochs + 1):\n",
    "        t_start = time.perf_counter()\n",
    "\n",
    "        train_loss = train(model, train_loader, loss_function, optimizer, device)\n",
    "        valid_acc = binary_acc(model, valid_loader,device)\n",
    "\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.synchronize()\n",
    "\n",
    "        t_end = time.perf_counter()\n",
    "        time_p.append(t_end)\n",
    "        loss_p.append(train_loss)\n",
    "        tr_acc_array.append(valid_acc)\n",
    "\n",
    "        print('Epoch: {:03d}, Loss: {:.8f}, Valid Acc: {:.8f}, Duration: {:.2f}'.format(\n",
    "            epoch, train_loss, valid_acc, t_end - t_start))\n",
    "\n",
    "# save model params\n",
    "torch.save(model.state_dict(), 'taskA_transformer_params_6_2.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### TaskB\n",
    "Since the data for taskB is imbalanced, add a weight for the loss of each sample according to its label during the training time:  \n",
    "{label:weight}={0.0: 1.9907674552798615, 1.0: 2.7555910543130993, 2.0:12.23404255319149, 3.0: 18.852459016393443}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 16\n",
    "trainB_data = IronyDataset(trainB, seq_to_tensor, train_feature)\n",
    "trainB_loader = DataLoader(trainB_data, batch_size=BATCH_SIZE, shuffle=False, collate_fn=collate_fn)\n",
    "\n",
    "validB_data = IronyDataset(validB, seq_to_tensor, train_feature)\n",
    "validB_loader = DataLoader(validB_data, batch_size=BATCH_SIZE, shuffle=False, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_class = 4\n",
    "model2 = SelfAttenClassifier(\n",
    "    encoder = Encoder(EncoderLayer(dim_model,attn_layer,ff_layer),num_encoder_layers),\n",
    "    classifier = SoftMax(classfier_input_dim,num_class),\n",
    "    )\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "if torch.cuda.device_count() > 1:\n",
    "    model2 = nn.DataParallel(model2)\n",
    "model2.to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "weight = torch.tensor([1.9907674552798615, 2.7555910543130993, 12.23404255319149, 18.852459016393443]).to(device)\n",
    "# loss_function = nn.NLLLoss(weight)\n",
    "loss_function = nn.NLLLoss()\n",
    "\n",
    "time_p, tr_acc_array, ts_acc, loss_p = [], [], [], []\n",
    "epochs = 10\n",
    "# running epoches\n",
    "for epoch in range(1, epochs + 1):\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.synchronize()\n",
    "\n",
    "        t_start = time.perf_counter()\n",
    "\n",
    "        train_loss = train(model2, trainB_loader, loss_function, optimizer, device)\n",
    "        valid_acc = binary_acc(model2, trainB_loader, device)\n",
    "\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.synchronize()\n",
    "\n",
    "        t_end = time.perf_counter()\n",
    "        time_p.append(t_end)\n",
    "        loss_p.append(train_loss)\n",
    "        tr_acc_array.append(valid_acc)\n",
    "\n",
    "        print('Epoch: {:03d}, Loss: {:.8f}, Valid Acc: {:.8f}, Duration: {:.2f}'.format(\n",
    "            epoch, train_loss, valid_acc, t_end - t_start))\n",
    "\n",
    "# save model params\n",
    "torch.save(model.state_dict(), 'taskB_transformer_params_6_2.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### generate predicted labels\n",
    "wirte predicted labels for test data:(one sample a line): label+\\t+orinignal word list\\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 1\n",
    "test_data = IronyDataset(test, seq_to_tensor, test_feature, with_label=False)\n",
    "test_loader = DataLoader(test_data, batch_size=BATCH_SIZE, shuffle=False, collate_fn=collate_fn)\n",
    "i=0\n",
    "result_f = open('result_a','w')\n",
    "for data in test_loader:\n",
    "    x, _, lengths, addition_feats = data\n",
    "    x = x.to(device)\n",
    "    addition_feats = addition_feats.to(device)\n",
    "    maxlen = x.size(1)\n",
    "    mask = None\n",
    "    sentence = \" \".join(test[i][\"word\"])\n",
    "#     print(x.size(), addition_feats.size())\n",
    "    pred, seq_feats = model(x, mask, addition_feats)\n",
    "    label = torch.max(pred, 1)[1].data.item()\n",
    "    result_f.write(str(label)+\"\\t\"+sentence+\"\\n\")\n",
    "    i+=1\n",
    "    \n",
    "result_f.close()\n",
    "print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 1\n",
    "test_data = IronyDataset(test, seq_to_tensor, test_feature, with_label=False)\n",
    "test_loader = DataLoader(test_data, batch_size=BATCH_SIZE, shuffle=False, collate_fn=collate_fn)\n",
    "i=0\n",
    "result_f = open('result_b','w')\n",
    "for data in test_loader:\n",
    "    x, _, lengths, addition_feats = data\n",
    "    x = x.to(device)\n",
    "    addition_feats = addition_feats.to(device)\n",
    "    maxlen = x.size(1)\n",
    "    mask = None\n",
    "    sentence = \" \".join(test[i][\"word\"])\n",
    "#     print(x.size(), addition_feats.size())\n",
    "    pred, seq_feats = model2(x, mask, addition_feats)\n",
    "    label = torch.max(pred, 1)[1].data.item()\n",
    "    result_f.write(str(label)+\"\\t\"+sentence+\"\\n\")\n",
    "    i+=1\n",
    "    \n",
    "result_f.close()\n",
    "print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run evaluation.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "全self-attention层可能存在的问题：\n",
    "- only capture the inner structure of a sentence, the relations between sentence parts and classification are not captured directly.\n",
    "- position information is not sufficiently modeled.\n",
    "\n",
    "因此经常self-attention层会和RNN/LSTM结合使用"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reference\n",
    "- [The Annotated Transformer](http://nlp.seas.harvard.edu/2018/04/03/attention.html#position-wise-feed-forward-networks)\n",
    "- [Multi-head Self Attention for Text Classification](https://www.kaggle.com/fareise/multi-head-self-attention-for-text-classification)\n",
    "- [BERT_pytorch](https://github.com/codertimo/BERT-pytorch)\n",
    "- [TextClassificationBenchmark](https://github.com/wabyking/TextClassificationBenchmark)"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,py:light"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
